{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "idx = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "batch_size = 48\n",
    "iterations = 9000\n",
    "\n",
    "i = 0\n",
    "alpha=0.5\n",
    "model.cuda()\n",
    "for e in range(iterations):\n",
    "    \n",
    "    ypred = model.forward(Variable(X_train[i:i+batch_size].cuda()))\n",
    "    \n",
    "    loss = loss_func(ypred, y_train[i:i+batch_size])\n",
    "    \n",
    "    i = (i+batch_size)%X_train.shape[0]\n",
    "    \n",
    "    if(e == 6000):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reg_loss = None\n",
    "    for param in model.parameters():\n",
    "        if reg_loss is None:\n",
    "            reg_loss = (alpha/2) * torch.sum(param**2)\n",
    "        else:\n",
    "            reg_loss = reg_loss + (alpha/2) * param.norm(2)**2\n",
    "\n",
    "    loss += lmbd * reg_loss\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"iteration {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "idx = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "batch_size = 48\n",
    "iterations = 9000\n",
    "\n",
    "i = 0\n",
    "alpha=0.5\n",
    "model.cuda()\n",
    "for e in range(iterations):\n",
    "    \n",
    "    ypred = model.forward(Variable(X_train[i:i+batch_size].cuda()))\n",
    "    \n",
    "    loss = loss_func(ypred, y_train[i:i+batch_size])\n",
    "    \n",
    "    i = (i+batch_size)%X_train.shape[0]\n",
    "    \n",
    "    if(e == 6000):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reg_loss = None\n",
    "    for param in model.parameters():\n",
    "        if reg_loss is None:\n",
    "            reg_loss = (alpha/2) * torch.sum(param)\n",
    "        else:\n",
    "            reg_loss = reg_loss + (alpha/2) * param.norm(1)\n",
    "\n",
    "    loss += lmbd * reg_loss\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"iteration {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2-SP regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = torch.load(weight_url)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "idx = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "batch_size = 48\n",
    "iterations = 9000\n",
    "\n",
    "i = 0\n",
    "alpha=0.5\n",
    "model.cuda()\n",
    "for e in range(iterations):\n",
    "    \n",
    "    ypred = model.forward(Variable(X_train[i:i+batch_size].cuda()))\n",
    "    \n",
    "    loss = loss_func(ypred, y_train[i:i+batch_size])\n",
    "    \n",
    "    i = (i+batch_size)%X_train.shape[0]\n",
    "    \n",
    "    if(e == 6000):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reg_loss = None\n",
    "    for name,param in model.parameters():\n",
    "        w0 = pretrained_weights[name].data\n",
    "        if reg_loss is None:\n",
    "            reg_loss = (alpha/2) * torch.sum((param-w0)**2)\n",
    "        else:\n",
    "            reg_loss = reg_loss + (alpha/2) * (param-w0).norm(2)**2\n",
    "\n",
    "    loss += lmbd * reg_loss\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"iteration {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1-SP regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = torch.load(weight_url)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "idx = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "X_train = X_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "batch_size = 48\n",
    "iterations = 9000\n",
    "\n",
    "i = 0\n",
    "alpha=0.5\n",
    "model.cuda()\n",
    "for e in range(iterations):\n",
    "    \n",
    "    ypred = model.forward(Variable(X_train[i:i+batch_size].cuda()))\n",
    "    \n",
    "    loss = loss_func(ypred, y_train[i:i+batch_size])\n",
    "    \n",
    "    i = (i+batch_size)%X_train.shape[0]\n",
    "    \n",
    "    if(e == 6000):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reg_loss = None\n",
    "    for name,param in model.parameters():\n",
    "        w0 = pretrained_weights[name].data\n",
    "        if reg_loss is None:\n",
    "            reg_loss = (alpha/2) * torch.sum((param-w0))\n",
    "        else:\n",
    "            reg_loss = reg_loss + (alpha/2) * (param-w0).norm(1)\n",
    "\n",
    "    loss += lmbd * reg_loss\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"iteration {}\".format(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
